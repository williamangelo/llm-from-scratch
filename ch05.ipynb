{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c7745154",
   "metadata": {},
   "source": [
    "EVALS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ab5a2cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import numpy\n",
    "import tiktoken\n",
    "import torch\n",
    "# import tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b79eb799",
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": 50257,\n",
    "    \"context_length\": 256,\n",
    "    \"emb_dim\": 768,\n",
    "    \"n_heads\": 12,\n",
    "    \"n_layers\": 12,\n",
    "    \"drop_rate\": 0.1,\n",
    "    \"qkv_bias\": False\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e18e1a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llm_from_scratch import GPTModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c6ed02fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(123)\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "model.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53f5810c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "from llm_from_scratch import generate_simple_text\n",
    "\n",
    "def text_to_token_ids(text, tokenizer):\n",
    "    encoded = tokenizer.encode(text, allowed_special={\"<|endoftext|>\"})\n",
    "    encoded_tensor = torch.tensor(encoded).unsqueeze(0) # add batch dimension\n",
    "    return encoded_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "10333d99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[6109, 3626, 6100,  345]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_context = \"Every effort moves you\"\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "text_to_token_ids(start_context, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f3f1d7b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def token_ids_to_text(token_ids, tokenizer):\n",
    "    flat = token_ids.squeeze(0)\n",
    "    return tokenizer.decode(flat.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6f623116",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = generate_simple_text(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(start_context, tokenizer),\n",
    "    max_new_tokens=10,\n",
    "    context_size=GPT_CONFIG_124M[\"context_length\"]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "54af333b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Every effort moves you rentingetic wasnÙ… refres RexAngel infieldcigans'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_ids_to_text(out, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f0ba544",
   "metadata": {},
   "source": [
    "CROSS ENTROPY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "de1e33e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = torch.tensor ([\n",
    "    [16833, 3626, 6100],\n",
    "    [40, 1107, 588]\n",
    "])\n",
    "\n",
    "targets = torch.tensor ([\n",
    "    [3626, 6100, 345],\n",
    "    [1107, 588, 11311]\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "58d84c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    logits = model(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a0d09d1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 50257])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c2c2500e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 50257])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probas = torch.softmax(logits, dim=-1)\n",
    "probas.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "eb5e9fd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token IDs: tensor([[[16657],\n",
      "         [  339],\n",
      "         [42826]],\n",
      "\n",
      "        [[49906],\n",
      "         [29669],\n",
      "         [41751]]])\n"
     ]
    }
   ],
   "source": [
    "token_ids = torch.argmax(probas, dim=-1, keepdim=True)\n",
    "print(f\"Token IDs: {token_ids}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "53354cfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target batch:  effort moves you\n",
      "Output batch:  Armed heNetflix\n"
     ]
    }
   ],
   "source": [
    "print(f\"Target batch: {token_ids_to_text(targets[0], tokenizer)}\")\n",
    "print(f\"Output batch: {token_ids_to_text(token_ids[0].flatten(), tokenizer)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4ee989b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "logits_flat = logits.flatten(0, 1)\n",
    "targets_flat = targets.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4d83cb09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(10.8009)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.nn.functional.cross_entropy(logits_flat, targets_flat)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11eafcb6",
   "metadata": {},
   "source": [
    "TRAIN AND VALIDATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d13856aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "\n",
    "file_path = \"data/the_verdict.txt\"\n",
    "url = \"https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch02/01_main-chapter-code/the-verdict.txt\"\n",
    "\n",
    "if not os.path.exists(file_path):\n",
    "    response = requests.get(url, timeout=30)\n",
    "    response.raise_for_status()\n",
    "    text_data = response.text\n",
    "    with open(file_path, \"w\", encoding=\"utf-8\") as file:\n",
    "        file.write(text_data)\n",
    "else:\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        text_data = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fcbcd83b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I HAD always thought Jack Gisburn rather a cheap genius'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_data[:55]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "003a0b98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20479, 5145)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_characters = len(text_data)\n",
    "total_tokens = len(tokenizer.encode(text_data))\n",
    "\n",
    "total_characters, total_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f8dd981",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llm_from_scratch import create_dataloader_v1, generate_simple_text\n",
    "\n",
    "train_ratio = 0.9\n",
    "split_idx = int(train_ratio * len(text_data))\n",
    "train_data = text_data[:split_idx]\n",
    "val_data = text_data[split_idx:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e5d9ed7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "train_loader = create_dataloader_v1(\n",
    "    train_data,\n",
    "    batch_size=2,\n",
    "    max_length=GPT_CONFIG_124M[\"context_length\"],\n",
    "    stride=GPT_CONFIG_124M[\"context_length\"],\n",
    "    drop_last=True,\n",
    "    shuffle=True,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "val_loader = create_dataloader_v1(\n",
    "    val_data,\n",
    "    batch_size=2,\n",
    "    max_length=GPT_CONFIG_124M[\"context_length\"],\n",
    "    stride=GPT_CONFIG_124M[\"context_length\"],\n",
    "    drop_last=False,\n",
    "    shuffle=False,\n",
    "    num_workers=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9a14eb23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loader\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n"
     ]
    }
   ],
   "source": [
    "print(\"Train loader\")\n",
    "for x, y in train_loader:\n",
    "    # pass\n",
    "    print(x.shape, y.shape)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f93187f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_loss_batch(input_batch, target_batch, model, device):\n",
    "    input_batch, target_batch = input_batch.to(device), target_batch.to(device)\n",
    "    logits = model(input_batch)\n",
    "    loss = torch.nn.functional.cross_entropy(logits.flatten(0, 1), target_batch.flatten())\n",
    "    return loss\n",
    "\n",
    "\n",
    "def calc_loss_loader(data_loader, model, device, num_batches=None):\n",
    "    total_loss = 0.\n",
    "    if len(data_loader) == 0:\n",
    "        return float(\"nan\")\n",
    "    elif num_batches is None:\n",
    "        num_batches = len(data_loader)\n",
    "    else:\n",
    "        # Reduce the number of batches to match the total number of batches in the data loader\n",
    "        # if num_batches exceeds the number of batches in the data loader\n",
    "        num_batches = min(num_batches, len(data_loader))\n",
    "    for i, (input_batch, target_batch) in enumerate(data_loader):\n",
    "        if i < num_batches:\n",
    "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
    "            total_loss += loss.item()\n",
    "        else:\n",
    "            break\n",
    "    return total_loss / num_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "06a97724",
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "device = \"cpu\"\n",
    "# model.to(device);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8fcbc7c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 10.988949563768175\n",
      "Validation loss: 10.97446346282959\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "with torch.no_grad():\n",
    "    train_loss = calc_loss_loader(train_loader, model, device)\n",
    "    val_loss = calc_loss_loader(val_loader, model, device)\n",
    "\n",
    "print(f\"Training loss: {train_loss}\")\n",
    "print(f\"Validation loss: {val_loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "506e8179",
   "metadata": {},
   "source": [
    "TRAININGGG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d142a0af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_simple(model, train_loader, \n",
    "    val_loader, optimizer, device, num_epochs,\n",
    "    eval_freq, eval_iter, start_context, tokenizer):\n",
    "    # Initialize lists to track losses and tokens seen\n",
    "    train_losses, val_losses, track_tokens_seen = [], [], []\n",
    "    tokens_seen, global_step = 0, -1\n",
    "\n",
    "    # Main training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()  # Set model to training mode\n",
    "        \n",
    "        for input_batch, target_batch in train_loader:\n",
    "            optimizer.zero_grad() # Reset loss gradients from previous batch iteration\n",
    "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
    "            loss.backward() # Calculate loss gradients\n",
    "            optimizer.step() # Update model weights using loss gradients\n",
    "            tokens_seen += input_batch.numel()\n",
    "            global_step += 1\n",
    "\n",
    "            # Optional evaluation step\n",
    "            if global_step % eval_freq == 0:\n",
    "                train_loss, val_loss = evaluate_model(\n",
    "                    model, train_loader, val_loader, device, eval_iter)\n",
    "                train_losses.append(train_loss)\n",
    "                val_losses.append(val_loss)\n",
    "                track_tokens_seen.append(tokens_seen)\n",
    "                print(f\"Ep {epoch+1} (Step {global_step:06d}): \"\n",
    "                      f\"Train loss {train_loss:.3f}, Val loss {val_loss:.3f}\")\n",
    "\n",
    "        # Print a sample text after each epoch\n",
    "        generate_and_print_sample(\n",
    "            model, tokenizer, device, start_context\n",
    "        )\n",
    "\n",
    "    return train_losses, val_losses, track_tokens_seen\n",
    "\n",
    "\n",
    "def evaluate_model(model, train_loader, val_loader, device, eval_iter):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        train_loss = calc_loss_loader(train_loader, model, device, num_batches=eval_iter)\n",
    "        val_loss = calc_loss_loader(val_loader, model, device, num_batches=eval_iter)\n",
    "    model.train()\n",
    "    return train_loss, val_loss\n",
    "\n",
    "\n",
    "def generate_and_print_sample(model, tokenizer, device, start_context, max_new_tokens=50):\n",
    "    model.eval()\n",
    "    context_size = model.pos_emb.weight.shape[0]\n",
    "    encoded = text_to_token_ids(start_context, tokenizer).to(device)\n",
    "    with torch.no_grad():\n",
    "        token_ids = generate_simple_text(\n",
    "            model=model, idx=encoded,\n",
    "            max_new_tokens=max_new_tokens, context_size=context_size\n",
    "        )\n",
    "    decoded_text = token_ids_to_text(token_ids, tokenizer)\n",
    "    print(decoded_text.replace(\"\\n\", \" \"))  # Compact print format\n",
    "    model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d0432d4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(123)\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "model.to(\"cpu\");\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.0004, weight_decay=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "067d158e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 1 (Step 000000): Train loss 9.764, Val loss 9.903\n",
      "Ep 1 (Step 000005): Train loss 8.115, Val loss 8.337\n",
      "Every effort moves you,,,,,,,,,,,,.                                     \n",
      "Ep 2 (Step 000010): Train loss 6.668, Val loss 7.049\n",
      "Ep 2 (Step 000015): Train loss 5.982, Val loss 6.624\n",
      "Every effort moves you, and, and, and, and, and, and, and, and,, and,,,,, and, and,, and, and, and,, and, and,, and, and,, and,, and\n",
      "Ep 3 (Step 000020): Train loss 5.689, Val loss 6.482\n",
      "Ep 3 (Step 000025): Train loss 5.687, Val loss 6.481\n",
      "Every effort moves you, and, and the the of the the the the to the the.                                   \n",
      "Ep 4 (Step 000030): Train loss 5.380, Val loss 6.435\n",
      "Ep 4 (Step 000035): Train loss 4.965, Val loss 6.374\n",
      "Every effort moves you his \"\"I and I had been the picture the picture and I had the picture, and I had been the                           \n",
      "Ep 5 (Step 000040): Train loss 4.693, Val loss 6.249\n",
      "Every effort moves you, and I had been a, and I was a a a his.                                   \n",
      "Ep 6 (Step 000045): Train loss 4.043, Val loss 6.261\n",
      "Ep 6 (Step 000050): Train loss 3.694, Val loss 6.231\n",
      "Every effort moves you know the \"I was a little the picture, and I had been him.     \"Oh, and I felt, and I had the end't, and he was, and I had been the fact, and I had\n",
      "Ep 7 (Step 000055): Train loss 3.381, Val loss 6.174\n",
      "Ep 7 (Step 000060): Train loss 3.224, Val loss 6.179\n",
      "Every effort moves you know it was not to the picture.                                          \n",
      "Ep 8 (Step 000065): Train loss 2.559, Val loss 6.127\n",
      "Ep 8 (Step 000070): Train loss 2.154, Val loss 6.192\n",
      "Every effort moves you know,\" was one of the picture for a smile that, and in a little.                                 \n",
      "Ep 9 (Step 000075): Train loss 1.789, Val loss 6.163\n",
      "Ep 9 (Step 000080): Train loss 1.387, Val loss 6.214\n",
      "Every effort moves you?\"  \"Yes--quite insensible to the irony.               \"Oh, and I had been at my elbow and I had the donkey.      \n",
      "Ep 10 (Step 000085): Train loss 1.089, Val loss 6.247\n",
      "Every effort moves you?\"  \"Yes--quite insensible to the fact with the last word.          \"Oh, and back the window-curtains, I saw that, and down the room, stopping now\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "train_losses, val_losses, tokens_seen = train_model_simple(\n",
    "    model, train_loader, val_loader, optimizer, device,\n",
    "    num_epochs=num_epochs, eval_freq=5, eval_iter=5,\n",
    "    start_context=\"Every effort moves you\", tokenizer=tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c24694ec",
   "metadata": {},
   "source": [
    "/\\ overfiting - val loss is stuck while train loss goes down"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a1991607",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "He was the only course I meant to do\n"
     ]
    }
   ],
   "source": [
    "generate_and_print_sample(\n",
    "    model, tokenizer, device, \"He was the only\",\n",
    "    max_new_tokens=5\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e58c8dfb",
   "metadata": {},
   "source": [
    "this function retrieves the token with highest probability, so it will essentially memorize the training set and overfit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dac8026b",
   "metadata": {},
   "source": [
    "DECODING STRATEGIES TO CONTROL RANDOMNESS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2017fc33",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mmodel\u001b[49m.to(\u001b[33m\"\u001b[39m\u001b[33mcpu\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      2\u001b[39m model.eval()\n\u001b[32m      4\u001b[39m tokenizer = tiktoken.get_encoding(\u001b[33m\"\u001b[39m\u001b[33mgpt2\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "model.to(\"cpu\")\n",
    "model.eval()\n",
    "\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "token_ids = generate_simple_text(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(\"Every effort moves you\", tokenizer),\n",
    "    max_new_tokens=25,\n",
    "    context_size=GPT_CONFIG_124M[\"context_length\"]\n",
    ")\n",
    "print(token_ids_to_text(token_ids=token_ids, tokenizer=tokenizer))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4de23dad",
   "metadata": {},
   "source": [
    "TEMPERATURE SCALING"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fd62b5f",
   "metadata": {},
   "source": [
    "use multinomial sampling to not \"brute force\" the most probable token, but instead sample based on the probability of each token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "cb4dc428",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_with_temperature(logits, temperature):\n",
    "    scaled_logits = logits / temperature\n",
    "    return torch.softmax(scaled_logits, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "0fa5892a",
   "metadata": {},
   "outputs": [],
   "source": [
    "temperatures = [1, 0.1, 5]\n",
    "test_logits = torch.tensor(\n",
    "    [4.51, 0.89, -11.90, 6.75, 1.63, -1.62, -1.89, 6.28, 1.79]\n",
    ")\n",
    "\n",
    "\n",
    "scaled_probas = [softmax_with_temperature(test_logits, T) for T in temperatures]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "bbf1b9d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([6.0913e-02, 1.6314e-03, 4.5492e-09, 5.7218e-01, 3.4193e-03, 1.3258e-04,\n",
       "         1.0121e-04, 3.5761e-01, 4.0126e-03]),\n",
       " tensor([1.8530e-10, 3.5189e-26, 0.0000e+00, 9.9099e-01, 5.7569e-23, 4.4220e-37,\n",
       "         2.9718e-38, 9.0133e-03, 2.8514e-22]),\n",
       " tensor([0.1606, 0.0779, 0.0060, 0.2514, 0.0903, 0.0471, 0.0447, 0.2288, 0.0932])]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaled_probas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b46c7f1",
   "metadata": {},
   "source": [
    "increasing temperature allow less likely values to be scaled up and adds \"creativity\" to the model output, lower temperature doubles down on most likely probas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63d3728a",
   "metadata": {},
   "source": [
    "top k sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a282b58",
   "metadata": {},
   "source": [
    "minimizes the probability of generating garbage in exchange of creativity - masks low probas with -inf so they become 0 after softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "f0fa7ab3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 4.5100,  0.8900, -1.9000,  6.7500,  1.6300, -1.6200, -1.8900,  6.2800,\n",
       "         1.7900])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_logits = torch.tensor(\n",
    "    [4.51, 0.89, -1.90, 6.75, 1.63, -1.62, -1.89, 6.28, 1.79]\n",
    ")\n",
    "test_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "07bfac42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([6.7500, 6.2800, 4.5100]), tensor([3, 7, 0]))"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_k = 3\n",
    "top_logits, top_pos = torch.topk(test_logits, top_k)\n",
    "top_logits, top_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "852b4664",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_logits = torch.where(\n",
    "    condition=test_logits < top_logits[-1], #top_logits[-1] is already the lowest of the top k values\n",
    "    input=torch.tensor(float(\"-inf\")),\n",
    "    other=test_logits\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "a3b751c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0615, 0.0000, 0.0000, 0.5775, 0.0000, 0.0000, 0.0000, 0.3610, 0.0000])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.softmax(new_logits, dim=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4629275",
   "metadata": {},
   "source": [
    "MODIFYING TEXT GENERATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "0cbb84f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, idx, max_new_tokens, context_size, temperature=0.0, top_k=None, eos_id=None):\n",
    "\n",
    "    # For-loop is the same as before: Get logits, and only focus on last time step\n",
    "    for _ in range(max_new_tokens):\n",
    "        idx_cond = idx[:, -context_size:]\n",
    "        with torch.no_grad():\n",
    "            logits = model(idx_cond)\n",
    "        logits = logits[:, -1, :]\n",
    "\n",
    "        # New: Filter logits with top_k sampling\n",
    "        if top_k is not None:\n",
    "            # Keep only top_k values\n",
    "            top_logits, _ = torch.topk(logits, top_k)\n",
    "            min_val = top_logits[:, -1]\n",
    "            logits = torch.where(logits < min_val, torch.tensor(float(\"-inf\")).to(logits.device), logits)\n",
    "\n",
    "        # New: Apply temperature scaling\n",
    "        if temperature > 0.0:\n",
    "            logits = logits / temperature\n",
    "\n",
    "            # New (not in book): numerical stability tip to get equivalent results on mps device\n",
    "            # subtract rowwise max before softmax\n",
    "            logits = logits - logits.max(dim=-1, keepdim=True).values\n",
    "            \n",
    "            # Apply softmax to get probabilities\n",
    "            probs = torch.softmax(logits, dim=-1)  # (batch_size, context_len)\n",
    "\n",
    "            # Sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)  # (batch_size, 1)\n",
    "\n",
    "        # Otherwise same as before: get idx of the vocab entry with the highest logits value\n",
    "        else:\n",
    "            idx_next = torch.argmax(logits, dim=-1, keepdim=True)  # (batch_size, 1)\n",
    "\n",
    "        if idx_next == eos_id:  # Stop generating early if end-of-sequence token is encountered and eos_id is specified\n",
    "            break\n",
    "\n",
    "        # Same as before: append sampled index to the running sequence\n",
    "        idx = torch.cat((idx, idx_next), dim=1)  # (batch_size, num_tokens+1)\n",
    "\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "0fdc8858",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Every effort moves you in,\" was to surprise, a little to have been the end of my\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "token_ids = generate_text(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(\"Every effort moves you\", tokenizer),\n",
    "    max_new_tokens=15,\n",
    "    context_size=GPT_CONFIG_124M[\"context_length\"],\n",
    "    top_k=25,\n",
    "    temperature=1.4\n",
    ")\n",
    "print(token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f6d9c1b",
   "metadata": {},
   "source": [
    "LOADING AND SAVING MODEL WEIGHTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "d3baa99c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to save\n",
    "torch.save(model.state_dict(), \"model.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "86bb6504",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to load\n",
    "loaded_model = GPTModel(GPT_CONFIG_124M)\n",
    "\n",
    "device = torch.device(\"cpu\")\n",
    "loaded_model.load_state_dict(torch.load(\"model.pth\", map_location=device))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c24a1f7",
   "metadata": {},
   "source": [
    "LOADING PRETRAINED WEIGHTS FROM OPENAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "2b875f12",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gpt_download import download_and_load_gpt2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1dc1b09c",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'download_and_load_gpt2' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m seetings, params = \u001b[43mdownload_and_load_gpt2\u001b[49m(\n\u001b[32m      2\u001b[39m     model_size=\u001b[33m\"\u001b[39m\u001b[33m124M\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m      3\u001b[39m     models_dir=\u001b[33m\"\u001b[39m\u001b[33mdata/models/gpt2\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      4\u001b[39m )\n",
      "\u001b[31mNameError\u001b[39m: name 'download_and_load_gpt2' is not defined"
     ]
    }
   ],
   "source": [
    "seetings, params = download_and_load_gpt2(\n",
    "    model_size=\"124M\",\n",
    "    models_dir=\"data/models/gpt2\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6745dbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llm_from_scratch import GPTModel\n",
    "\n",
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": 50257,\n",
    "    \"context_length\": 256,\n",
    "    \"emb_dim\": 768,\n",
    "    \"n_heads\": 12,\n",
    "    \"n_layers\": 12,\n",
    "    \"drop_rate\": 0.1,\n",
    "    \"qkv_bias\": False\n",
    "}\n",
    "\n",
    "model_configs = {\n",
    "    \"gpt2-small (124M)\": {\"emb_dim\": 768, \"n_layers\": 12, \"n_heads\": 12},\n",
    "    \"gpt2-medium (355M)\": {\"emb_dim\": 1024, \"n_layers\": 24, \"n_heads\": 16},\n",
    "    \"gpt2-large (774M)\": {\"emb_dim\": 1280, \"n_layers\": 36, \"n_heads\": 20},\n",
    "    \"gpt2-xl (1558M)\": {\"emb_dim\": 1600, \"n_layers\": 48, \"n_heads\": 25},\n",
    "}\n",
    "\n",
    "# Copy the base configuration and update with specific model settings\n",
    "model_name = \"gpt2-medium (355M)\"  # Example model name\n",
    "NEW_CONFIG = GPT_CONFIG_124M.copy()\n",
    "NEW_CONFIG.update(model_configs[model_name])\n",
    "NEW_CONFIG.update({\"context_length\": 1024, \"qkv_bias\": True})\n",
    "\n",
    "gpt = GPTModel(NEW_CONFIG)\n",
    "gpt.eval();\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a1cfa5ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign(left, right):\n",
    "    if left.shape != right.shape:\n",
    "        raise ValueError(\"Shape mismatch\")\n",
    "    return torch.nn.Parameter(torch.tensor(right))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "1a1b548b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def load_weights_into_gpt(gpt, params):\n",
    "    gpt.pos_emb.weight = assign(gpt.pos_emb.weight, params['wpe'])\n",
    "    gpt.tok_emb.weight = assign(gpt.tok_emb.weight, params['wte'])\n",
    "    \n",
    "    for b in range(len(params[\"blocks\"])):\n",
    "        q_w, k_w, v_w = np.split(\n",
    "            (params[\"blocks\"][b][\"attn\"][\"c_attn\"])[\"w\"], 3, axis=-1)\n",
    "        gpt.trf_blocks[b].att.W_query.weight = assign(\n",
    "            gpt.trf_blocks[b].att.W_query.weight, q_w.T)\n",
    "        gpt.trf_blocks[b].att.W_key.weight = assign(\n",
    "            gpt.trf_blocks[b].att.W_key.weight, k_w.T)\n",
    "        gpt.trf_blocks[b].att.W_value.weight = assign(\n",
    "            gpt.trf_blocks[b].att.W_value.weight, v_w.T)\n",
    "\n",
    "        q_b, k_b, v_b = np.split(\n",
    "            (params[\"blocks\"][b][\"attn\"][\"c_attn\"])[\"b\"], 3, axis=-1)\n",
    "        gpt.trf_blocks[b].att.W_query.bias = assign(\n",
    "            gpt.trf_blocks[b].att.W_query.bias, q_b)\n",
    "        gpt.trf_blocks[b].att.W_key.bias = assign(\n",
    "            gpt.trf_blocks[b].att.W_key.bias, k_b)\n",
    "        gpt.trf_blocks[b].att.W_value.bias = assign(\n",
    "            gpt.trf_blocks[b].att.W_value.bias, v_b)\n",
    "\n",
    "        gpt.trf_blocks[b].att.out_proj.weight = assign(\n",
    "            gpt.trf_blocks[b].att.out_proj.weight, \n",
    "            params[\"blocks\"][b][\"attn\"][\"c_proj\"][\"w\"].T)\n",
    "        gpt.trf_blocks[b].att.out_proj.bias = assign(\n",
    "            gpt.trf_blocks[b].att.out_proj.bias, \n",
    "            params[\"blocks\"][b][\"attn\"][\"c_proj\"][\"b\"])\n",
    "\n",
    "        gpt.trf_blocks[b].ff.layers[0].weight = assign(\n",
    "            gpt.trf_blocks[b].ff.layers[0].weight, \n",
    "            params[\"blocks\"][b][\"mlp\"][\"c_fc\"][\"w\"].T)\n",
    "        gpt.trf_blocks[b].ff.layers[0].bias = assign(\n",
    "            gpt.trf_blocks[b].ff.layers[0].bias, \n",
    "            params[\"blocks\"][b][\"mlp\"][\"c_fc\"][\"b\"])\n",
    "        gpt.trf_blocks[b].ff.layers[2].weight = assign(\n",
    "            gpt.trf_blocks[b].ff.layers[2].weight, \n",
    "            params[\"blocks\"][b][\"mlp\"][\"c_proj\"][\"w\"].T)\n",
    "        gpt.trf_blocks[b].ff.layers[2].bias = assign(\n",
    "            gpt.trf_blocks[b].ff.layers[2].bias, \n",
    "            params[\"blocks\"][b][\"mlp\"][\"c_proj\"][\"b\"])\n",
    "\n",
    "        gpt.trf_blocks[b].norm1.scale = assign(\n",
    "            gpt.trf_blocks[b].norm1.scale, \n",
    "            params[\"blocks\"][b][\"ln_1\"][\"g\"])\n",
    "        gpt.trf_blocks[b].norm1.shift = assign(\n",
    "            gpt.trf_blocks[b].norm1.shift, \n",
    "            params[\"blocks\"][b][\"ln_1\"][\"b\"])\n",
    "        gpt.trf_blocks[b].norm2.scale = assign(\n",
    "            gpt.trf_blocks[b].norm2.scale, \n",
    "            params[\"blocks\"][b][\"ln_2\"][\"g\"])\n",
    "        gpt.trf_blocks[b].norm2.shift = assign(\n",
    "            gpt.trf_blocks[b].norm2.shift, \n",
    "            params[\"blocks\"][b][\"ln_2\"][\"b\"])\n",
    "\n",
    "    gpt.final_norm.scale = assign(gpt.final_norm.scale, params[\"g\"])\n",
    "    gpt.final_norm.shift = assign(gpt.final_norm.shift, params[\"b\"])\n",
    "    gpt.out_head.weight = assign(gpt.out_head.weight, params[\"wte\"])\n",
    "    \n",
    "    \n",
    "load_weights_into_gpt(gpt, params)\n",
    "gpt.to(\"cpu\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "30b69dec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I would only call for his presence to do the world and our country together. But my answer to a second term on the campaign trail will\n"
     ]
    }
   ],
   "source": [
    "# torch.manual_seed(11)\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "\n",
    "token_ids = generate_text(\n",
    "    model=gpt,\n",
    "    idx=text_to_token_ids(\"I would only\", tokenizer).to(\"cpu\"),\n",
    "    max_new_tokens=25,\n",
    "    context_size=NEW_CONFIG[\"context_length\"],\n",
    "    top_k=50,\n",
    "    temperature=1.4\n",
    ")\n",
    "print(token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11cd39c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bb75806",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44a5bb35",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-from-scratch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
